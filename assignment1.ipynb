{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Q1. \n",
    "\n",
    "1. Artifical Intelligence:- Smart application that can perform tasks without any human intervention.\n",
    "\n",
    "Ex- Alexa \n",
    "\n",
    "2. Machine Learning - It provide statistical, visualization, predicative modles forecasting.\n",
    "\n",
    "Eg:- Customer Feedback Analysis\n",
    "\n",
    "3. Deep Learning - It like an human brain that how it works and posses information.\n",
    "\n",
    "Eg: Object Detection \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.\n",
    "\n",
    "Supervised Learning in which we train a model from that we predict the output behavior.\n",
    "\n",
    "Eg:- House Price Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Q3.\n",
    "\n",
    "Unsupervised Learning in which we makes cluster of data to predict the desired outcome. In case we do not \n",
    "already know the outcome.\n",
    "\n",
    "Eg:- Customer Feedback to predict the watch sales in a mall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. \n",
    "\n",
    "AI is the overarching concept of creating intelligent machines, ML is a subset of AI focused on developing algorithms that learn from data, DL is a subset of ML utilizing deep neural networks for complex pattern recognition, and DS is an interdisciplinary field that involves extracting insights and knowledge from data using various techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.\n",
    "\n",
    "Supervised Learning in which we already know what desired outcome would be come.\n",
    "\n",
    "Unsupervised Learning in that case we do not know what outcome would come here we make clusters to predict the desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.\n",
    "\n",
    "Training Dataset:- We train our ML modal from this dataset.\n",
    "\n",
    "Validation Dataset:- We do Hyper Tuning in the dataset.\n",
    "\n",
    "Test Dataset:- We perform test do calculate accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7.\n",
    "\n",
    "Unsupervised learning can be used in anomaly detection by allowing algorithms to identify patterns and deviations in data without the need for explicit labels or pre-classified examples. Anomaly detection involves finding instances in a dataset that differ significantly from the majority of the data points. Unsupervised learning methods are particularly well-suited for this task because anomalies are often rare and may not be well-represented in labeled training dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8.\n",
    "\n",
    "\n",
    "**Supervised Learning Algorithms:**\n",
    "\n",
    "1. **Linear Regression**: A regression algorithm used for predicting continuous numerical values based on input features.\n",
    "\n",
    "2. **Logistic Regression**: Used for binary classification problems, it estimates the probability that a given input point belongs to a certain class.\n",
    "\n",
    "3. **Decision Trees**: A tree-like model where decisions are made based on feature values at each node, used for both classification and regression.\n",
    "\n",
    "4. **Random Forest**: An ensemble method that creates multiple decision trees and combines their predictions for improved accuracy and generalization.\n",
    "\n",
    "5. **Support Vector Machines (SVM)**: Used for both classification and regression tasks by finding the optimal hyperplane that separates different classes or predicts values.\n",
    "\n",
    "6. **Naive Bayes**: A probabilistic classifier that is based on Bayes' theorem and is commonly used for text classification and spam filtering.\n",
    "\n",
    "7. **K-Nearest Neighbors (KNN)**: Classifies data points based on the majority class among their k-nearest neighbors in the feature space.\n",
    "\n",
    "8. **Gradient Boosting Algorithms (e.g., XGBoost, LightGBM)**: Ensemble methods that build multiple weak learners sequentially to improve overall prediction performance.\n",
    "\n",
    "9. **Neural Networks**: Deep learning models with interconnected layers of nodes that can handle complex patterns and features in data.\n",
    "\n",
    "**Unsupervised Learning Algorithms:**\n",
    "\n",
    "1. **K-Means Clustering**: Divides data points into clusters based on similarity, where each data point belongs to the cluster with the nearest mean.\n",
    "\n",
    "2. **Hierarchical Clustering**: Builds a tree of clusters by iteratively merging or splitting them based on similarity.\n",
    "\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Clusters data points based on density, identifying regions of high density separated by regions of low density.\n",
    "\n",
    "4. **Gaussian Mixture Models (GMM)**: Represents data as a mixture of several Gaussian distributions, often used for density estimation and clustering.\n",
    "\n",
    "5. **Principal Component Analysis (PCA)**: Reduces the dimensionality of data while retaining as much variance as possible, often used for feature extraction.\n",
    "\n",
    "6. **Autoencoders**: Neural network architectures used for unsupervised feature learning and dimensionality reduction.\n",
    "\n",
    "7. **Isolation Forest**: An ensemble-based method that isolates anomalies by randomly partitioning data points.\n",
    "\n",
    "8. **One-Class SVM**: A variant of SVM used for anomaly detection by identifying the optimal hyperplane that separates normal data from anomalies.\n",
    "\n",
    "9. **Local Outlier Factor (LOF)**: Measures the local density deviation of a data point with respect to its neighbors to identify anomalies.\n",
    "\n",
    "10. **t-SNE (t-distributed Stochastic Neighbor Embedding)**: Dimensionality reduction technique used for visualization by capturing the high-dimensional data structure in a lower-dimensional space.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
